{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning - Bike Forecasting download model and leverage in notebook\n",
    "\n",
    "- Initially you will connect using the Kernal -> Python 3.10 SDK V2\n",
    "- Then you will use the project_environment\n",
    "\n",
    "**BikeShare Demand Forecasting**\n",
    "\n",
    "Recall: https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-automated-ml-forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required to set to your AutoML Job Name\n",
    "job_name = 'careful_jelly_wk9dj1jssn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml import Input\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<>\"\n",
    "    resource_group = \"<>\"\n",
    "    workspace = \"<>\"\n",
    "\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = workspace.resource_group\n",
    "output[\"Location\"] = workspace.location\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Retrieve the Best Trial (Best Model's trial/run)\n",
    "Use the MLFLowClient to access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Initialize MLFlow Client\n",
    "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface. \n",
    "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
    "\n",
    "*IMPORTANT*, you need to have installed the latest MLFlow packages with:\n",
    "\n",
    "    pip install azureml-mlflow\n",
    "\n",
    "    pip install mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the tracking URI for MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Obtain the tracking URL from MLClient\n",
    "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
    "    name=ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MLFLOW TRACKING URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the AutoML parent Job\n",
    "\n",
    "- Set your job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_name)\n",
    "# Example if providing an specific Job name/ID\n",
    "# job_name = \"591640e8-0f88-49c5-adaa-39b9b9d75531\"\n",
    "\n",
    "# Get the parent run\n",
    "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
    "\n",
    "print(\"Parent Run: \")\n",
    "print(mlflow_parent_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
    "print(mlflow_parent_run.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the AutoML best child run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model's child run\n",
    "\n",
    "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
    "print(\"Found best child run id: \", best_child_run_id)\n",
    "\n",
    "best_run = mlflow_client.get_run(best_child_run_id)\n",
    "\n",
    "print(\"Best child run: \")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Get best model run's validation metrics\n",
    "\n",
    "Access the results (such as models, artifacts, metrics) of a previously completed AutoML Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_run.data.metrics, index=[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model evaluation and deployemnt.\n",
    "## 6.1 Download the best model\n",
    "\n",
    "Access the results (such as models, artifacts, metrics) of a previously completed AutoML Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local folder\n",
    "import os\n",
    "\n",
    "local_dir = \"./artifact_downloads/\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download run's artifacts/outputs\n",
    "local_path = mlflow_client.download_artifacts(\n",
    "    best_run.info.run_id, \"outputs\", local_dir\n",
    ")\n",
    "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the contents of the MLFlow model folder\n",
    "os.listdir(\"./artifact_downloads/outputs/mlflow-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization\n",
    "We can look at the engineered feature names generated in time-series featurization via the JSON file named 'engineered_feature_names.json' under the run outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(local_path, \"engineered_feature_names.json\"), \"r\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View featurization summary\n",
    "You can also see what featurization steps were performed on different raw features in the user data. For each raw feature in the user data, the following information is displayed:\n",
    "\n",
    "+ Raw feature name\n",
    "+ Number of engineered features formed out of this raw feature\n",
    "+ Type detected\n",
    "+ If feature was dropped\n",
    "+ List of feature transformations for the raw feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the JSON as a pandas DataFrame\n",
    "with open(os.path.join(local_path, \"featurization_summary.json\"), \"r\") as f:\n",
    "    records = json.load(f)\n",
    "fs = pd.DataFrame.from_records(records)\n",
    "\n",
    "# View a summary of the featurization\n",
    "fs[\n",
    "    [\n",
    "        \"RawFeatureName\",\n",
    "        \"TypeDetected\",\n",
    "        \"Dropped\",\n",
    "        \"EngineeredFeatureCount\",\n",
    "        \"Transformations\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Track - Loading and testing best model by locally downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Show the contents of the MLFlow model folder\n",
    "os.listdir(\"./artifact_downloads/outputs/mlflow-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    \"./test_dataset/bike-no-test.csv\"\n",
    ").reset_index(drop=True)\n",
    "y_actual = test_df.pop(\"cnt\").values\n",
    "\n",
    "test_df.shape, y_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['date'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment from conda.yaml file and activate in notebook to leverage.\n",
    "\n",
    "```\n",
    "cd BlackHillsEnergy/BHE_automl-forecasting-task-bike-share/\n",
    "cd 'artifact_downloads'\n",
    "cd outputs/mlflow-model/\n",
    "conda env create -f conda.yaml\n",
    "conda activate project_environment\n",
    "ipython kernel install --user --name project_environment --display-name \"project_environment\"\n",
    "pip install azure-ai-ml\n",
    "pip install mlflow\n",
    "pip install azureml-mlflow\n",
    "pip install --upgrade plotly\n",
    "```\n",
    "\n",
    "- After the environment has been made available to Jupyter, Refresh this session (F5, or Hit refresh on your browser)\n",
    "\n",
    "When you go to your `Kernel` -> `Change Kernel`, it will be available to select. You will have to rerun the notebook, but when you download the model, you will be using all of the correct versions of libraries.\n",
    "\n",
    "*Note to remove an environment with conda leverage\n",
    "\n",
    "```conda env remove -n job_env```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MLFlow model from the downloaded MLFlow model files\n",
    "\n",
    "model = mlflow.sklearn.load_model(\"./artifact_downloads/outputs/mlflow-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forecasting models predict with .forecast() or .forecast_quantiles(), not with .predict()\n",
    "\n",
    "# y_preds = model.forecast(test_df, ignore_data_errors=True)\n",
    "\n",
    "# y_preds\n",
    "\n",
    "# # Original forecasting with .forecast_quantiles(X_test)\n",
    "# # https://github.com/Azure/azureml-examples/blob/main/python-sdk/tutorials/automl-with-azureml/forecasting-energy-demand/forecasting_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting from trained model\n",
    "\n",
    "[https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-forecast-function/auto-ml-forecasting-function.ipynb]\n",
    "\n",
    "2 scenerios:\n",
    "- 1. Right after training data\n",
    "- 2. More complex - forecasting when tereh is a gap between training and testing data\n",
    "\n",
    "### Scenerio One\n",
    "\n",
    "we have time to retrain the model every time we wish to forecast.  Forecasts that are made on daily and slower cadence.\n",
    "Retrain the model every time benefits the accuracy because the most recent data is often most informative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./predict_no_gap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_test = pd.read_csv(\"./test_dataset/bike-no-test.csv\", parse_dates=[model.time_column_name])\n",
    "y_test = X_test.pop(\"cnt\").values\n",
    "\n",
    "y_pred_no_gap, xy_nogap =  model.forecast(X_test)\n",
    "xy_nogap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Forecasting model may be used for the prediction of forecasting intervals by running forecast_quantiles(). This method accepts the same parameters as forecast()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confidence intervals\n",
    "\n",
    "quantiles = model.forecast_quantiles(X_test)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution forecasts\n",
    "\n",
    "Often the figure of interest is not just the point prediction, but the prediction at some quantile of the distribution. This arises when the forecast is used to control some kind of inventory, for example of grocery items or virtual machines for a cloud service. In such case, the control point is usually something like \"we want the item to be in stock and not run out 99% of the time\". This is called a \"service level\". Here is how you get quantile forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which quantiles you would like\n",
    "model.quantiles = [0.01, 0.5, 0.95]\n",
    "# use forecast_quantiles function, not the forecast() one\n",
    "y_pred_quantiles = model.forecast_quantiles(X_test)\n",
    "\n",
    "# quantile forecasts returned in a Dataframe along with the time and time series id columns\n",
    "y_pred_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination-date forecast: \"just do something\"\n",
    "\n",
    "In some scenarios, the **X_test** is not known. The forecast is likely to be weak, because it is missing contemporaneous predictors, which we will need to impute. If you still wish to predict forward under the assumption that the last known values will be carried forward, you can forecast out to \"destination date\". The destination date still needs to fit within the forecast horizon from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = max(X_test['date'])\n",
    "print(dest)\n",
    "\n",
    "y_pred_dest, xy_dest = model.forecast(forecast_destination=dest)\n",
    "\n",
    "# This form also shows how we imputed the predictors which were not given. (Not so well! Use with caution!)\n",
    "xy_dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting away from training data\n",
    "\n",
    "Suppose we trained a model, some time passed, and now we want to apply the model without re-training. If the model \"looks back\" -- uses previous values of the target -- then we somehow need to provide those values to the model.\n",
    "\n",
    "**Won't cover right now, but good to know that it is an option**\n",
    "\n",
    "[https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-forecast-function/auto-ml-forecasting-function.ipynb]\n",
    "\n",
    "\n",
    "![image info](./ForecastAwayfromTraining.png)\n",
    "\n",
    "The notion of forecast origin comes into play: the forecast origin is **the last period for which we have seen the target value**. This applies per time-series, so each time-series can have a different forecast origin.\n",
    "\n",
    "The part of data before the forecast origin is the **prediction context**. To provide the context values the model needs when it looks back, we pass definite values in `y_test` (aligned with corresponding times in `X_test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling Forecast\n",
    "\n",
    " \n",
    "[https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast#evaluating-model-accuracy-with-a-rolling-forecast]\n",
    "\n",
    "A best practice procedure is a rolling evaluation that rolls the trained forecaster forward in time over the test set, averaging error metrics over several prediction windows. Ideally, the test set for the evaluation is long relative to the model's forecast horizon. Estimates of forecasting error may otherwise be statistically noisy and, therefore, less reliable.\n",
    "\n",
    "[https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast#forecasting-with-a-trained-model]\n",
    "\n",
    "### Often used for evaluation\n",
    "\n",
    "we use **known actual values** of the target for our context data\n",
    "\n",
    "The step size for the **rolling forecast** is set to one which means that the forecaster is advanced one period, or one day in our demand prediction example, at each iteration. The total number of forecasts returned by rolling_forecast depends on the length of the test set and this step size.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a rolling forecast, advancing the forecast origin by 1 period on each iteration through the test set\n",
    "X_test = pd.read_csv(\"./test_dataset/bike-no-test.csv\", parse_dates=[model.time_column_name])\n",
    "y_test = X_test.pop(\"cnt\").values\n",
    "    \n",
    "X_rf = model.rolling_forecast(X_test, y_test, step=1, ignore_data_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions, actuals, and horizon relative to rolling origin to the test feature data\n",
    "assign_dict = {\n",
    "            model.forecast_origin_column_name: \"forecast_origin\",\n",
    "            model.forecast_column_name: \"predicted\",\n",
    "            model.actual_column_name: \"cnt\",\n",
    "        }\n",
    "X_rf.rename(columns=assign_dict, inplace=True)\n",
    "# drop rows where prediction or actuals are nan happens because of missing actuals or at edges of time due to lags/rolling windows]\n",
    "X_rf.dropna(inplace=True)\n",
    "print(f\"The predictions have {X_rf.shape[0]} rows and {X_rf.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing\n",
    "\n",
    "This is like forecast using the X_test (since there is no gap in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_gap, xy_nogap = model.forecast(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_nogap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction into the future\n",
    "\n",
    "\n",
    "Confidence interval and distributional forecasts\n",
    "\n",
    "AutoML cannot currently estimate forecast errors beyond the forecast horizon set during training, so the forecast_quantiles() function will return missing values for quantiles not equal to 0.5 beyond the forecast horizon.\n",
    "\n",
    "\n",
    "**forecast_quanties()** generates forecasts for given quanties of the prediction distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.5, 0.975]\n",
    "predicted_column_name = \"predicted\"\n",
    "PI = \"prediction_interval\"\n",
    "model.quantiles = quantiles\n",
    "pred_quantiles = model.forecast_quantiles(X_test, ignore_data_errors=True)\n",
    "pred_quantiles[PI] = pred_quantiles[[min(quantiles), max(quantiles)]].apply(lambda x: \"[{}, {}]\".format(x[0], x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = \"cnt\"\n",
    "X_test[target_column_name] = y_test\n",
    "X_test[PI] = pred_quantiles[PI]\n",
    "X_test[predicted_column_name] = pred_quantiles[0.5]\n",
    "# drop rows where prediction or actuals are nan\n",
    "# happens because of missing actuals\n",
    "# or at edges of time due to lags/rolling windows\n",
    "clean = X_test[X_test[[target_column_name, predicted_column_name]].notnull().all(axis=1)]\n",
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single point Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_test = pd.read_csv(\"./test_dataset/bike-no-test.csv\", parse_dates=[model.time_column_name])\n",
    "y_test = X_test.pop(\"cnt\").values\n",
    "\n",
    "label_query = y_test.copy().astype(np.float)\n",
    "label_query.fill(np.nan)\n",
    "\n",
    "#single point prediction\n",
    "df = model.forecast_quantiles(forecast_destination=pd.Timestamp(2012, 9, 2))\n",
    "\n",
    "# Get forecasts for the 5th, 50th, and 90th percentiles \n",
    "model.quantiles = [0.05, 0.5, 0.9]\n",
    "df2 = model.forecast_quantiles(forecast_destination=pd.Timestamp(2013, 12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "jialiu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "BikeShare"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "file_extension": ".py",
  "framework": [
   "Azure ML AutoML"
  ],
  "friendly_name": "Forecasting BikeShare Demand",
  "index_order": 1,
  "kernelspec": {
   "display_name": "project_environment",
   "language": "python",
   "name": "project_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "tags": [
   "Forecasting"
  ],
  "task": "Forecasting",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "7db8a53542ba35b01ae7762868d8d16c0f1f14c2eb043bc8fff69addb7ba1f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
